{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crawlcomment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oSS-J-0BqBW",
        "colab_type": "code",
        "outputId": "036b5a87-fab3-42b4-e162-eb09cec815cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!pip install langdetect \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (1.0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpChOmQ6-tW-",
        "colab_type": "code",
        "outputId": "e2b561fd-749a-4f1f-9dd1-d5808cd660c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from langdetect import detect\n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "from dateutil.parser import parse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUwSl0biA8mL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "article_array = [\"sports\",\"economy\",\"bangladesh\",\"opinion\",\"life-style\",\"entertainment\",\"pachmisheli\",\"international\"]\n",
        "monthDictionary = {\"জানুয়ারি\":1,\"ফেব্রুয়ারি\":2,\"মার্চ\":3,\"এপ্রিল\":4,\"মে\":5,\"জুন\":6,\"জুলাই\":7,\"আগস্ট\":8,\"সেপ্টেম্বর\":9,\"অক্টোবর\":10,\"নভেম্বর\":11,\"ডিসেম্বর\":12}\n",
        "digitDictionary = {\"০\":0,\"১\":1,\"২\":2,\"৩\":3,\"৪\":4,\"৫\":5,\"৬\":6,\"৭\":7,\"৮\":8,\"৯\":9}\n",
        "\n",
        "published_date = \"\"\n",
        "topic_title = \"\"\n",
        "topic_description = \"\"\n",
        "detail_data_url = \"\"\n",
        "\n",
        "list_comment = []\n",
        "list_commenter_name = []\n",
        "list_comment_url = []\n",
        "list_commenter_device = []\n",
        "list_commenter_id = []\n",
        "list_news_title = []\n",
        "list_news_published_date = []\n",
        "list_news_description = []\n",
        "list_news_content_url = []\n",
        "list_total_comment_in_this_news = []\n",
        "Total_topic_comment = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpF0NkwHBCTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trade_spider(max_pages,topic):\n",
        "    global Total_topic_comment\n",
        "    page = 1\n",
        "    while page <= max_pages:\n",
        "        Total_topic_comment = 0\n",
        "        url = 'https://www.prothomalo.com/'+topic+'/article/?page=' + str(page)\n",
        "#         print(\"{} Article url\\n\".format(topic))\n",
        "        print(url)\n",
        "        source_code = requests.get(url, params=None)\n",
        "        plain_text = source_code.text\n",
        "        soup = BeautifulSoup(plain_text, features=\"lxml\")\n",
        "        for link in soup.findAll('a', {'class': 'link_overlay'}):\n",
        "            href = link.get('href')\n",
        "            if topic not in href:\n",
        "                continue\n",
        "            get_single_page_comments(href, str(page),topic)\n",
        "        page += 1\n",
        "        print(\"Total Topic  {}\".format(Total_topic_comment))\n",
        "#     print(\"length of list_comment {} , list_commenter_name {} list_comment_url {} list_commenter_device {} \"\n",
        "#           \"list_news_title {} list_news_published_date {} list_news_description {} list_news_content_url {} list_commenter_id {} list_total_comment_in_this_news {}\"\n",
        "#           .format(len(list_comment), len(list_commenter_name), len(list_comment_url), len(list_commenter_device),\n",
        "#                   len(list_news_title),\n",
        "#                   len(list_news_published_date), len(list_news_description), len(list_news_content_url),\n",
        "#                   len(list_commenter_id),len(list_total_comment_in_this_news)))\n",
        "\n",
        "#     print(\"Commeter names \\n\", list_commenter_name)\n",
        "    \n",
        "    \n",
        "    myDataFrame = pd.DataFrame(list_news_published_date,columns=['Date']) \n",
        "    myDataFrame [\"Comment\"] = list_comment\n",
        "    myDataFrame [\"Commenter_name\"] = list_commenter_name\n",
        "    myDataFrame [\"Comment_json_url\"] = list_comment_url\n",
        "    myDataFrame [\"Commenter_device\"] = list_commenter_device\n",
        "    myDataFrame [\"Commenter_id\"] = list_commenter_id\n",
        "    myDataFrame [\"Title\"] = list_news_title\n",
        "    myDataFrame [\"Description\"] = list_news_description\n",
        "    myDataFrame [\"content_url\"] = list_news_content_url\n",
        "    myDataFrame [\"Total_Comment_on_this_news\"] = list_total_comment_in_this_news\n",
        "    \n",
        "    return myDataFrame\n",
        "\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEkuoS_aYM5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def convertToEnglishDate(publishedDateinBangla):\n",
        "  nltk_tokens = nltk.word_tokenize(publishedDateinBangla)\n",
        "#   print (nltk_tokens)\n",
        "  date = \"\"\n",
        "  month = \"\"\n",
        "  year = \"\"\n",
        "\n",
        "  for i,word in enumerate(nltk_tokens[:3]):\n",
        "  #   print(\"GE\",i,word)\n",
        "    if (i == 0):\n",
        "      for c in word:\n",
        "#         print(digitDictionary[c])\n",
        "        date += str(digitDictionary[c])\n",
        "      pass\n",
        "    elif (i == 1):\n",
        "      month = str(monthDictionary[word])\n",
        "#       print(monthDictionary[word])\n",
        "    else:\n",
        "      for c in word:\n",
        "#         print(digitDictionary[c])\n",
        "        year += str(digitDictionary[c])\n",
        "\n",
        "  return date+\"-\"+month+\"-\"+year\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_Q5FDUNBHp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_single_page_comments(item_url, page,topic):\n",
        "    global translator\n",
        "    global detail_data_url\n",
        "    # print(\"Item url\\n\")\n",
        "    # print(\"https://www.prothomalo.com{}\".format(item_url))\n",
        "    detail_data_url = \"https://www.prothomalo.com\"+item_url\n",
        "    found_item = re.search('/'+topic+'/article/(.+?)/', item_url)\n",
        "    # print(\"Found item\")\n",
        "\n",
        "    if found_item:\n",
        "        global published_date\n",
        "        global topic_title\n",
        "        global topic_description\n",
        "        global list_comment\n",
        "        global list_commenter_name\n",
        "        global list_comment_url\n",
        "        global list_commenter_device\n",
        "        global list_news_title\n",
        "        global list_news_published_date\n",
        "        global list_news_description\n",
        "        global list_news_content_url\n",
        "        global list_commenter_id\n",
        "        global Total_topic_comment\n",
        "        global list_total_comment_in_this_news\n",
        "\n",
        "        content_id = found_item.group(1)\n",
        "        # print(\"Comment json url\\n\")\n",
        "        # print('https://www.prothomalo.com/api/comments/get_comments_json/?content_id=' + content_id)\n",
        "        response = requests.get('https://www.prothomalo.com/api/comments/get_comments_json/?content_id=' + content_id)\n",
        "        commenter_json_url = 'https://www.prothomalo.com/api/comments/get_comments_json/?content_id=' + content_id\n",
        "        json_data = response.json()\n",
        "        # print(\"Json type \\n\",type(json_data))\n",
        "#         print(\"{} People commented \\n\".format(len(json_data)))\n",
        "        Total_topic_comment += 1\n",
        "        if len(json_data) >= 1:\n",
        "\n",
        "            try:\n",
        "                source_code = requests.get(detail_data_url, params=None)\n",
        "                plain_text = source_code.text\n",
        "                soup = BeautifulSoup(plain_text, features=\"lxml\")\n",
        "                topic_title = soup.title.get_text()\n",
        "                # print(\"Topic title \\n\",topic_title)\n",
        "                topic_description = soup.find(itemprop=\"articleBody\").get_text()\n",
        "\n",
        "                # print(\"description \\n\", topic_description)\n",
        "                published_date = soup.find(itemprop=\"datePublished\").get_text()\n",
        "                # print(\"Date \\n\",published_date )\n",
        "                # published_date = published_date.date()\n",
        "                # print(published_date)\n",
        "\n",
        "            except:\n",
        "                print(\"Exception in souping\")\n",
        "\n",
        "\n",
        "        nonword_pattern = re.compile('[.@_!#$%^&*()<>?/|}{~:DpP0123456789 ১২৩৪৫৬৭৮৯০]*$')\n",
        "\n",
        "        for comment_id in json_data:\n",
        "            comment_object = json_data[comment_id]\n",
        "            comment = comment_object['comment']\n",
        "            commenter_name = comment_object['commenter_name']\n",
        "            device = comment_object['device']\n",
        "\n",
        "            if not nonword_pattern.fullmatch(comment):\n",
        "\n",
        "                if ' ' not in comment:\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    lang = detect(comment)\n",
        "\n",
        "                    if lang == \"bn\" or lang == \"en\":\n",
        "                        formatted_comment = '---------------------------------' \\\n",
        "                                            + '\\nComment ID: ' + comment_id \\\n",
        "                                            + '\\nContent ID: ' + content_id \\\n",
        "                                            + '\\nPage no: ' + page + '\\n' + commenter_name + '\\n'+ device \\\n",
        "                                            + '\\n' + comment + '\\n\\n' + str(published_date)\n",
        "\n",
        "                        # print(formatted_comment)\n",
        "                        if commenter_name == \"hidden\":\n",
        "                            commenter_name = \"নাম প্রকাশে অনিচ্ছুক\"\n",
        "\n",
        "                        list_comment.append(comment)\n",
        "                        list_commenter_name.append(commenter_name)\n",
        "                        list_comment_url.append(commenter_json_url)\n",
        "                        list_commenter_device.append(device)\n",
        "                        list_news_title.append(topic_title)\n",
        "                        list_news_published_date.append(convertToEnglishDate(published_date))\n",
        "                        list_news_description.append(topic_description)\n",
        "                        list_news_content_url.append(detail_data_url)\n",
        "                        list_commenter_id.append(comment_id)\n",
        "                        list_total_comment_in_this_news.append(str(len(json_data)))\n",
        "                        # print(\"Total commec {}\".format(len(json_data)))\n",
        "                        # insert_data_to_file(project_name, formatted_comment)\n",
        "                except LangDetectException:\n",
        "                    pass\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGF2B8hTWzNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxyN-o7bBJqm",
        "colab_type": "code",
        "outputId": "92a9409c-756f-4ca5-abf1-e129a9db735e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# [0 \"sports\",1 \"economy\",2 \"bangladesh\", 3 \"opinion\", 4 \"life-style\", 5 \"entertainment\", 6 \"pachmisheli\",7 \"international\"]\n",
        "choose_topic = article_array[0]\n",
        "\n",
        "print(\"The topic you chose {}\".format(choose_topic))\n",
        "\n",
        "myDataFrame = trade_spider(3,choose_topic)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The topic you chose sports\n",
            "https://www.prothomalo.com/sports/article/?page=1\n",
            "Total Topic  21\n",
            "https://www.prothomalo.com/sports/article/?page=2\n",
            "Total Topic  19\n",
            "https://www.prothomalo.com/sports/article/?page=3\n",
            "Total Topic  19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hI7In9Q-l3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "myDataFrame.to_csv(choose_topic+\".csv\", encoding='utf-8', index=False)\n",
        "\n",
        "files.download(choose_topic+\".csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp4eO-j1fyr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}